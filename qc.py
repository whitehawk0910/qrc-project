# -*- coding: utf-8 -*-
"""qc.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wJzL6bddAincPY_R9HZTN0XQXJSlGAn6
"""

api_key="api-key "

from google.colab import drive
drive.mount('/content/drive')
import os
folder_path = '/content/drive/My Drive/statlog+german+credit+data'
os.chdir(folder_path)

import pandas as pd
import re

# Load german.data
file_path = 'german.data'
with open(file_path, 'r') as file:
    lines = file.readlines()

# Extract numerical data and ignore markers like 'Axx'
parsed_data = []
for line in lines:
    numbers = re.findall(r'\b\d+\b', line)
    parsed_data.append(list(map(int, numbers)))

# Create a DataFrame
data = pd.DataFrame(parsed_data)

if len(numbers) < 21:
    numbers.extend([float('nan')] * (21 - len(numbers)))
parsed_data.append([int(x) if pd.notna(x) else x for x in numbers]) # Use list comprehension with conditional conversion

# Create a DataFrame
data = pd.DataFrame(parsed_data)

# Assign column names
data.columns = [
    'Status', 'Duration', 'Credit_History', 'Purpose', 'Credit_Amount',
    'Savings', 'Employment', 'Installment_Rate', 'Personal_Status',
    'Other_Debtors', 'Residence_Since', 'Property', 'Age',
    'Other_Installment_Plans', 'Housing', 'Existing_Credits', 'Job',
    'Liable_People', 'Telephone', 'Foreign_Worker', 'Target'
]

data['Target'] = data['Target'].apply(lambda x: 1 if x == 1 else 0)

# Load german.data-numeric
numeric_data = pd.read_csv('german.data-numeric', delim_whitespace=True, header=None)

numeric_data.columns = [
    'Status', 'Duration', 'Credit_History', 'Purpose', 'Credit_Amount',
    'Savings', 'Employment', 'Installment_Rate', 'Personal_Status',
    'Other_Debtors', 'Residence_Since', 'Property', 'Age',
    'Other_Installment_Plans', 'Housing', 'Existing_Credits', 'Job',
    'Liable_People', 'Telephone', 'Foreign_Worker', 'Feature_21',
    'Feature_22', 'Feature_23', 'Feature_24', 'Target'
]

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(numeric_data.drop('Target', axis=1))
y = numeric_data['Target']

data.to_csv('german_data_cleaned.csv', index=False)
numeric_data.to_csv('german_data_numeric_cleaned.csv', index=False)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load cleaned data
data = pd.read_csv('german_data_numeric_cleaned.csv')

# Separate features and target
X = data.drop('Target', axis=1)
y = data['Target']

# Scale features for quantum processing
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

pip install qiskit qiskit-machine-learning scikit-learn pandas

pip install qiskit-aer

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report, recall_score
from qiskit.circuit.library import ZZFeatureMap
from qiskit.primitives import StatevectorSampler

# Load the cleaned numeric dataset
data_numeric = pd.read_csv('german_data_numeric_cleaned.csv')
X = data_numeric.iloc[:, :-1].values
y = data_numeric.iloc[:, -1].values

# Binary encoding of labels (adapt as needed for your dataset)
y = (y == 1).astype(int)

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Define the quantum feature map
feature_map = ZZFeatureMap(feature_dimension=X.shape[1], reps=2, entanglement='linear')

# StatevectorSampler
sampler = StatevectorSampler()

# Function to calculate kernel matrix
def calculate_kernel_matrix(X1, X2=None):
    """
    Calculates the kernel matrix for datasets X1 and X2 using the StatevectorSampler.
    If X2 is None, computes the kernel for X1 with itself.
    """
    if X2 is None:
        X2 = X1

    n1, n2 = len(X1), len(X2)
    kernel_matrix = np.zeros((n1, n2))

    # Iterate through all pairs (i, j)
    for i in range(n1):
        for j in range(i, n2):
            # Assign parameters to feature maps
            circuit1 = feature_map.assign_parameters(X1[i])
            circuit2 = feature_map.assign_parameters(X2[j])

            # Run both circuits with the StatevectorSampler
            result1 = sampler.run([circuit1]).result()
            result2 = sampler.run([circuit2]).result()

            # Extract statevectors
            statevector1 = result1.raw_results[0].data  # Access the raw statevector
            statevector2 = result2.raw_results[0].data  # Access the raw statevector

            # Compute fidelity
            fidelity = abs(np.vdot(statevector1, statevector2)) ** 2

            # Assign fidelity to the kernel matrix
            kernel_matrix[i, j] = fidelity
            if i != j:  # Symmetric matrix
                kernel_matrix[j, i] = fidelity

    return kernel_matrix

# Compute kernel matrices
quantum_kernel_matrix_train = calculate_kernel_matrix(X_train)
quantum_kernel_matrix_test = calculate_kernel_matrix(X_test, X_train)

# Train SVM
svc = SVC(kernel='precomputed')
svc.fit(quantum_kernel_matrix_train, y_train)

# Test the model
y_pred = svc.predict(quantum_kernel_matrix_test)

# Evaluate
print("Classification Report:")
print(classification_report(y_test, y_pred))
print(f"Recall Score: {recall_score(y_test, y_pred)}")

import numpy as np
import joblib  # For saving and loading the SVM model
import logging
from sklearn.metrics import accuracy_score

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Step 9: Save the trained SVM model
logging.info("Saving the trained SVM model...")
joblib.dump(svc, 'quantum_svm_model.pkl')
logging.info("Model saved as 'quantum_svm_model.pkl'.")

# Step 10: Analyze and save the performance metrics
# Calculate additional metrics
accuracy = accuracy_score(y_test, y_pred)

# Logging the performance
logging.info("Performance Metrics:")
logging.info(f"Accuracy Score: {accuracy}")
logging.info(f"Recall Score: {recall_score(y_test, y_pred)}")

# Save metrics to a file for reference
with open("performance_metrics.txt", "w") as metrics_file:
    metrics_file.write("Performance Metrics:\n")
    metrics_file.write(f"Accuracy Score: {accuracy}\n")
    metrics_file.write(f"Recall Score: {recall_score(y_test, y_pred)}\n")
    metrics_file.write("\nClassification Report:\n")
    metrics_file.write(classification_report(y_test, y_pred))

logging.info("Performance metrics saved to 'performance_metrics.txt'.")

# Step 11: Predict on new unseen data (if available)
# Assuming 'X_new' contains new samples for prediction
try:
    # Compute kernel matrix for new samples
    # X_new = ... (Add your new data loading here)
    quantum_kernel_matrix_new = quantum_kernel.evaluate(X_new, X_train)
    y_new_pred = svc.predict(quantum_kernel_matrix_new)

    # Save predictions
    np.savetxt("new_predictions.csv", y_new_pred, delimiter=",", header="Predictions", comments="")
    logging.info("Predictions for new data saved to 'new_predictions.csv'.")
except NameError:
    logging.warning("No new data provided for prediction. Skipping prediction step.")

# Step 12: Visualize results (Optional)
try:
    import matplotlib.pyplot as plt
    from sklearn.metrics import ConfusionMatrixDisplay

    # Plot confusion matrix
    disp = ConfusionMatrixDisplay.from_estimator(svc, quantum_kernel_matrix_test, y_test, cmap='viridis')
    plt.title("Confusion Matrix for Quantum SVM")
    plt.savefig("confusion_matrix.png")
    plt.show()
    logging.info("Confusion matrix saved as 'confusion_matrix.png'.")
except ImportError:
    logging.warning("Matplotlib not installed. Skipping visualization step.")

